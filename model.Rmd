---
title: "model"
author: "Rich"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This file builds multi-level regression models for the motor slowing and reward project with Jenny and co., using the pupil data.

This is just a demo/example of how you might fit hierarchical gams to pupil data with n=5.

# load the libraries that we will be using #

## install ##

```{r install-pkg}
# install.packages("remotes")
# remotes::install_github("stan-dev/cmdstanr")
# 
# install.packages("devtools")
# devtools::install_github("jmgirard/standist")
# 
# install.packages(c("tidyverse", "RColorBrewer", "patchwork", "brms",
#                    "tidybayes", "bayesplot", "future"))
```

take a snapshot of loaded packages and update the lock.file using renv

```{r snapshot-renv}
# take a snapshot and update the lock.file
# renv::snapshot() # this is only necessary when new packages or installed or packages are updated.
```

## load ##

```{r load-pkg}
pkg <- c("cmdstanr", "standist", "tidyverse", "RColorBrewer", "patchwork", 
         "brms", "tidybayes", "bayesplot", "future", "parallel", "rstan")

lapply(pkg, library, character.only = TRUE)
```

## settings ##

```{r set-options}
options(brms.backend = "cmdstanr",
        mc.cores = parallel::detectCores(),
        future.fork.enable = TRUE,
        future.rng.onMisuse = "ignore") ## automatically set in RStudio

rstan_options(auto_write = TRUE)

supportsMulticore()

detectCores()
```

## plot settings ##

theme settings for ggplot

```{r, eval = F}
theme_set(
  theme_bw() +
    theme(text = element_text(size = 18, face = "bold"), 
          title = element_text(size = 18, face = "bold"),
          legend.position = "bottom")
)

## Set the amount of dodge in figures
pd <- position_dodge(0.7)
pd2 <- position_dodge(1)
```

## read in data and create factors where necessary ##

```{r}
data <- read_csv("data/data_n5.csv") %>%
  mutate(pid = factor(pid),
         condition = factor(condition, 
                            levels = c("neutral", "reward")))
head(data)
str(data)

## 18068 datapoints
## 5 (pid) x 10 (trials per condition) x 2 (conditions) x 200 (samples: 40s x 5hz)
## 5*10*2*200= 20k

## 20000-18068=1932
## 400 datapoints removed due to whole trial exclusions e.g., 2 trials of data (200 per trial)
## 1932-400=1532 NaNs (0.2s windows of NaN)
```

Since the modelling takes an age and this is just an example, to use even less data, you could do the following and just focus on 10s in the middle (i.e., 5s before and after the experimental cue is displayed).

```{r}
## append m to denote middle of the samples
datam <- data %>%
  filter(time > 14999 & time < 25001)
head(datam)
str(datam)

## 4552 datapoints
write_csv(datam, "data/datam_n5.csv")
```


## load in previously saved models (as necessary) ##

This is useful if you want to look at aspects of a previously compiled model

```{r}
bp1 <- readRDS("models/bp1.rds")
```

# section 1 #

## quick plot to check the data look as they should ##

create summary data at the pid level

```{r}
data_pid <- datam %>%
  group_by(pid, condition, time) %>% 
  summarise(mean = mean(pupil, na.rm = TRUE),
            sd = sd(pupil, na.rm = TRUE),
            n = length(unique(trial/2)), # 
            sem = (sd/sqrt(length(unique(trial/2)))),
            ci = sem*1.96)
head(data_pid)
```

create summary data at the group level

```{r}
data_group <- datam %>%
  group_by(condition, time) %>% 
  summarise(mean = mean(pupil, na.rm = TRUE),
            sd = sd(pupil, na.rm = TRUE),
            n = length(unique(pid)), # n here is the total pids per condition
            sem = (sd/sqrt(length(unique(pid)))),
            ci = sem*1.96)
head(data_group)
```

## ribbon plot ##

with sem at the pid level

```{r}
p1.1 <- ggplot(data_pid,
               aes(x = time, y = mean, fill = condition)) +
  geom_line(aes(colour = condition), alpha = 1, linewidth = 1) +
  geom_ribbon(aes(ymin=mean-sem, ymax=mean+sem), alpha = 0.5) +
  geom_vline(xintercept = 20000, linetype = "dashed", colour = "darkgrey") +
  scale_colour_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") +
  scale_x_continuous(breaks=seq(15000,25000,5000),
                     labels= seq(15,25,5)) +
  scale_y_continuous(limits=c(-0.5, 1),
                     breaks = seq(-0.5, 1, 0.5)) +
  facet_wrap(~pid, nrow = 2) 
p1.1

ggsave("figures/datam_ribbon_n5_pid.jpeg",
       width = 10, height = 8, dpi = 800)
```

with sem at the group level

```{r}
p1.2 <- ggplot(data_group,
               aes(x = time, y = mean, fill = condition)) +
  geom_line(aes(colour = condition), alpha = 1, linewidth = 1) +
  geom_ribbon(aes(ymin=mean-sem, ymax=mean+sem), alpha = 0.5) +
  geom_vline(xintercept = 20000, linetype = "dashed", colour = "darkgrey") +
  scale_colour_brewer(palette = "Dark2") +
  scale_x_continuous(breaks=seq(15000,25000,5000),
                     labels= seq(15,25,5)) +
  scale_y_continuous(limits=c(-0.2, 0.6),
                     breaks = seq(-0.2, 0.6, 0.2)) +
  scale_fill_brewer(palette = "Dark2")
p1.2

ggsave("figures/datam_ribbon_n5_group.jpeg",
       width = 10, height = 8, dpi = 800)
```

# section 2 #

## build some models ##

## b0.1 - intercepts only ##

this is just to get things going i.e., a model without any predictors. And to act as a low-level reference.

## formula ##

```{r}
formula = bf(pupil ~ 1)
```

## check the priors available ##

```{r}
get_prior(formula,
          data = datam, family = gaussian())
```

## visualise priors ##

here we would normally visualise priors of interest to make a judgment about what would constitute weakly informative priors or generic weakly informative priors (according to the following weblink). The basic intuition is that the prior distribution should not be flat/uniform (like frequentist stats) as this is completely unreasonable that all values are equally likely in advance. We have too much domain knowledge for that to make sense. e.g., pupil diameter in this case. Also, in psychology and human neuro, we are far from being able to precisely specify informative priors (like, say, in physics maybe, where they have quantitative theories and precise predictions). So, weakly informative priors try to cover all reasonable values, given domain knowledge. The example below with pupil size might help. e.g., we know that human pupil size can only really cover certain values so let's set our priors to easily cover those values, and minimise implausibly large values (for example).

https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

a quick look at the ribbon plots from the wrangle script (and re-plotted above) shows a range from ~0-0.4 with 0.2 as the approximate centre. So let's plot some distributions.

```{r}
visualize("normal(0, 0.25)", "normal(0, 0.5)", "normal(0, 1)",
          xlim = c(-2,2))
```

(0, 0.25) seems reasonable for the intercept

same for sigma to start somewhere.

## set priors ##

```{r}
priors <- c(
  set_prior("normal(0, 0.25)", class = "Intercept"),
  set_prior("normal(0, 0.25)", class = "sigma")
)
```

## run the model ##

abbreivations for model names, as follows:

b=Bayesian
p=pupil

This shouldn't take too long to build. ~3s on my macbook pro.

```{r}
plan(multicore)
bp0 <- brm(formula = formula,
        data = datam, family = gaussian(),
        prior = priors,
        iter = 2000, warmup = 1000, cores = 8, chains = 4,
        save_pars = save_pars(all=TRUE),
        control = list(adapt_delta = 0.99),
        seed = 123,
        file = "models/bp0")
summary(bp0)
```

## take a look ##

chains

```{r}
plot(bp0)
```

pp check

```{r}
pp_bp0 <- pp_check(bp0, ndraws = 100)
pp_bp0
```


Ok, so that just shows that we can model the average pupil size. 

Nothing to learn there really, but it is useful to see that is working in model building world.


and now maybe skip any interim models and head straight towards a more complex model, just to see if it builds and how long it takes etc.

The model specification below for our hierarchical gam follows Gavin Simpson's stan forum post.
https://discourse.mc-stan.org/t/specifying-varying-effect-structures-with-hierarchical-generalised-additive-models-gams/35596/2


From Gavin Simpson:
>note the use of the xt argument to use a B-spline basis for the fs smooths, to match your specification from the other smooth.

>These two models account for the smooth time-treatment effects through the factor-by smooth of time by condition, with treatment means modelled through the condition parametric effect. The models differ in how they treat (penalize) the subject-specific smooths:

    >1. the first form fits a smooth of time for each subject (including random intercepts and linear slopes), where the penalties will shrink the subject specific curves towards their respective group means (the parametric condition effects), while
    2. the second form extends the first form in two ways:
    i. the subjects in each treatment group have a common wiggliness, but the wiggliness can vary between treatment groups , and
    ii. the penalties will shrink the curve towards their respective treatment-specific smooth

>The second form will also be quite a lot more complex to fit however.

In the next model, we will build option 1 above from Gavin Simpson.

## bp1 - smooth(time, by =condition) plus smooth(time, pid) ##

## formula ##

```{r}
formula = bf(pupil ~ 1 + condition +
               s(time, by = condition, bs = "bs", k = 10) +
               s(time, pid, bs = "fs", xt = list(bs = "bs")))
```

## check the priors available ##

```{r}
get_prior(formula,
          data = datam, family = gaussian())
```

ok, so there are more priors to set than the first model. 

A further intuition on priors is that I set the centre of the distribution on zero. This is especially important for effects of interest or slopes (e.g., class = "b"). In this case, the difference between reward and neutral, for example is centred on zero (no difference) and then as it moves away from zero the density reduces. This makes sense as in much of psych/human neuroscience, effects closer to zero (smaller effects) are more likely than larger effects, given what we know. 

## set priors ##

```{r}
priors <- c(
  set_prior("normal(0, 0.25)", class = "Intercept"),
  set_prior("normal(0, 0.25)", class = "b"),
  set_prior("normal(0, 0.25)", class = "sds"),
  set_prior("normal(0, 0.25)", class = "sigma")
)
```

## run the model ##

This will take some time to build and depend on your machine's skill set.
e.g., my macbook pro with 8 cores took... about 10 minutes

For the real deal, with N=24, plus 40s of data (and depending on the sampling rate), it would take several hours to run. And for dense sampling, it would take a day or two. So, kick it off overnight or the weekend and go to the pub for a beer.


```{r}
# this will help us track time
t1 <- Sys.time()

plan(multicore)
bp1 <- brm(formula = formula,
        data = datam, family = gaussian(),
        prior = priors,
        iter = 2000, warmup = 1000, cores = 8, chains = 4,
        save_pars = save_pars(all=TRUE),
        control = list(adapt_delta = 0.99),
        seed = 123,
        file = "models/bp1")
summary(bp1)

t2 <- Sys.time()

t2 - t1

# Time difference of ~ 10 minutes macbook pro.
```


# section 3 #

## model diagnostics ##

first take a look at the mixing of chains across posterior samples

```{r}
post <- as_draws_df(bp1)
str(post)
```

## look at the chains for the key variables of interest ##

```{r}
post1 <- post %>% 
  select(contains(c("b_", "bs_", "sds", "sigma", "b_Intercept", "chain"))) %>%  
  mutate(chain = .chain)
head(post1)
```

then plot them 

(they should look like furry caterpillars with a lot of overlap)

```{r} 
p_chains1 <- post1 %>% 
  mcmc_trace(facet_args = list(ncol = 4)) +
  scale_x_continuous(breaks = c(0, 1000)) +
  theme(legend.position = "bottom")
p_chains1

# save it
ggsave ("figures/chains.jpeg",
        width = 8, height = 6)
```

## other diagnostics ##

```{r}
# # these two below are worth reporting.
bp1_neff <- mcmc_plot(bp1, type = "neff")
bp1_neff
# 
bp1_rhat <- mcmc_plot(bp1, type = "rhat")
bp1_rhat
# 
bp1_diag <- bp1_neff / bp1_rhat
bp1_diag

## save it
ggsave("figures/diag.jpeg",
       width = 8, height = 6)
```